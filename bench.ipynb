{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# Initialize Tensorflow \n",
    "tf.keras.layers.Dense(100)\n",
    "from src.utils.gpu_memory_grow import gpu_memory_grow\n",
    "\n",
    "from src.utils import utils\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpu_memory_grow(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_SEQUENCES_FOLDER = \"data/generated\"\n",
    "CONTENT_DATA_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "\n",
    "SAVE_FOLDER = \"results/amplitudes\"\n",
    "\n",
    "STYLE_DATASETS = [\n",
    "        \"data/simulated_dataset/amplitude_shift/1.0_1.0.h5\", \n",
    "        \"data/simulated_dataset/amplitude_shift/2.0_2.0.h5\", \n",
    "        \"data/simulated_dataset/amplitude_shift/3.0_3.0.h5\", \n",
    "        \"data/simulated_dataset/amplitude_shift/4.0_4.0.h5\", \n",
    "        \"data/simulated_dataset/amplitude_shift/5.0_5.0.h5\", \n",
    "        \"data/simulated_dataset/amplitude_shift/6.0_6.0.h5\", \n",
    "        \"data/simulated_dataset/amplitude_shift/7.0_7.0.h5\" , \n",
    "        \"data/simulated_dataset/amplitude_shift/8.0_8.0.h5\" , \n",
    "        \"data/simulated_dataset/amplitude_shift/9.0_9.0.h5\" , \n",
    "        \"data/simulated_dataset/amplitude_shift/10.0_10.0.h5\"\n",
    "    ]\n",
    "\n",
    "STYLE_NAMES = [utils.get_name(f) for f in STYLE_DATASETS]\n",
    "\n",
    "SEQ_SHAPE = (64, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Generated Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_dataset(real_style_path:list, gen_folder:str=GENERATED_SEQUENCES_FOLDER):\n",
    "    filename = utils.get_name(real_style_path)\n",
    "    \n",
    "    train_gen_filepath = f\"{gen_folder}/{filename}_train.tfrecords\"\n",
    "    valid_gen_filepath = f\"{gen_folder}/{filename}_valid.tfrecords\"\n",
    "    \n",
    "    dset_train = tf.data.Dataset.load(train_gen_filepath)\n",
    "    dset_valid = tf.data.Dataset.load(valid_gen_filepath)\n",
    "    \n",
    "    dset_train = dset_train.unbatch().batch(64)\n",
    "    dset_valid = dset_valid.unbatch().batch(64)\n",
    "    \n",
    "    return dset_train, dset_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_fake_datasets(style_datasets:list):\n",
    "    real_style_dataset = {}\n",
    "    fake_style_dataset = {}\n",
    "    bs = 64 # args().simulated_arguments.batch_size\n",
    "\n",
    "    for style_path in style_datasets:\n",
    "        sty_name = utils.get_name(style_path)\n",
    "\n",
    "        dset_style_train, dset_style_valid = utils.load_dset(style_path, drop_labels=False, bs=bs)\n",
    "        dset_style_train = utils.extract_labels(dset_style_train)\n",
    "        dset_style_valid = utils.extract_labels(dset_style_valid)\n",
    "\n",
    "        stylized_train, stylized_valid = get_generated_dataset(style_path, GENERATED_SEQUENCES_FOLDER)\n",
    "\n",
    "        fake_style_dataset[f\"{sty_name}_train\"] = stylized_train\n",
    "        fake_style_dataset[f\"{sty_name}_valid\"] = stylized_valid\n",
    "        \n",
    "        real_style_dataset[f\"{sty_name}_train\"] = dset_style_train\n",
    "        real_style_dataset[f\"{sty_name}_valid\"] = dset_style_valid\n",
    "        \n",
    "    return real_style_dataset, fake_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_real, dsets_fake = get_real_fake_datasets(STYLE_DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Some Sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_comparison(real_dataset:dict, fake_dataset: dict, save_folder:str):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 20))\n",
    "\n",
    "    for i, style in enumerate(STYLE_DATASETS):\n",
    "        style_name = utils.get_name(style)\n",
    "        \n",
    "        real_sequence = next(iter(real_dataset[f'{style_name}_valid']))[0][0]\n",
    "        fake_sequence = next(iter(fake_dataset[f'{style_name}_valid']))[0][0]\n",
    "        \n",
    "        ax = plt.subplot(len(STYLE_DATASETS), 2, 2*i+1)\n",
    "        ax.set_title(f'Real Sequence Style {i+ 1}.')\n",
    "        ax.plot(real_sequence)\n",
    "        ax.set_ylim(-0.1, 20.1)\n",
    "        ax.grid()\n",
    "        \n",
    "        ax = plt.subplot(len(STYLE_DATASETS), 2, 2*i+2)\n",
    "        ax.set_title(f'Fake Sequence Style {i+ 1}.')\n",
    "        ax.plot(fake_sequence)\n",
    "        ax.set_ylim(-0.1, 20.1)\n",
    "        \n",
    "        ax.grid()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_folder}/generation_comparison.png\")\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_comparison(dsets_real, dsets_fake, SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute our simple Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval import simple_metric\n",
    "\n",
    "def get_batches(dset, n_batches):\n",
    "    _arr = np.array([c[0] for c in dset.take(n_batches)])\n",
    "    return _arr.reshape((-1, _arr.shape[-2], _arr.shape[-1]))\n",
    "\n",
    "\n",
    "def compute_metrics(dset_real, dset_fake, style_names, model_folder: str):\n",
    "    def time_shift_evaluation(big_batch):\n",
    "        return [simple_metric.estimate_time_shift(big_batch, 0, i) for i in range(big_batch.shape[-1])]\n",
    "    \n",
    "    real_noise_metric, gen_noise_metric = [], []\n",
    "    real_ampl_metric, gen_ampl_metric = [], []\n",
    "    real_ts_metric, gen_ts_metric = [], []\n",
    "\n",
    "    for style_name in style_names:\n",
    "        print(f\"[+] Compute metric for {style_name}\")\n",
    "        real_batch = get_batches(dset_real[f\"{style_name}_valid\"], 10)\n",
    "        fake_batch = get_batches(dset_fake[f\"{style_name}_valid\"], 10)\n",
    "        \n",
    "        real_noise_metric.append(simple_metric.simple_metric_on_noise(real_batch)[-1])\n",
    "        gen_noise_metric.append(simple_metric.simple_metric_on_noise(fake_batch)[-1])\n",
    "        \n",
    "        real_ampl_metric.append(simple_metric.extract_amplitude_from_signals(real_batch))\n",
    "        gen_ampl_metric.append(simple_metric.extract_amplitude_from_signals(fake_batch))\n",
    "        \n",
    "        real_ts_metric.append(time_shift_evaluation(real_batch))\n",
    "        gen_ts_metric.append(time_shift_evaluation(fake_batch))\n",
    "        \n",
    "    real_mean_noises = np.mean(real_noise_metric, axis=-1).reshape((-1, 1))\n",
    "    fake_mean_noises = np.mean(gen_noise_metric, axis=-1).reshape((-1, 1))\n",
    "    mean_noises = np.concatenate((real_mean_noises, fake_mean_noises), axis=-1)\n",
    "    \n",
    "    real_mean_ampl = np.mean(real_ampl_metric, axis=-1).reshape((-1, 1))\n",
    "    fake_mean_ampl = np.mean(gen_ampl_metric, axis=-1).reshape((-1, 1))\n",
    "    mean_ampl= np.concatenate((real_mean_ampl, fake_mean_ampl), axis=-1)\n",
    "    \n",
    "    real_mean_time_shift = np.mean(real_ts_metric, axis=-1).reshape((-1, 1))\n",
    "    fake_mean_time_shift = np.mean(gen_ts_metric, axis=-1).reshape((-1, 1))\n",
    "    mean_time_shift= np.concatenate((real_mean_time_shift, fake_mean_time_shift), axis=-1)\n",
    "    \n",
    "    df_noises = pd.DataFrame(data=mean_noises, index=style_names, columns=['Real', 'Fake'])\n",
    "    df_ampl = pd.DataFrame(data=mean_ampl, index=style_names, columns=['Real', 'Fake'])\n",
    "    df_time_shift = pd.DataFrame(data=mean_time_shift, index=style_names, columns=['Real', 'Fake'])\n",
    "    \n",
    "    df_noises.to_hdf(f'{model_folder}/noise_metric.h5', key=\"data\")\n",
    "    df_ampl.to_hdf(f'{model_folder}/ampl_metric.h5', key=\"data\")\n",
    "    df_time_shift.to_hdf(f'{model_folder}/time_shift_metric.h5', key=\"data\")\n",
    "    \n",
    "    return df_noises, df_ampl, df_time_shift\n",
    "\n",
    "def plot_metric(df_metric:pd.DataFrame, title, save_to):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    df_metric[\"Real\"].plot(ax=ax, style='.-')\n",
    "    df_metric[\"Fake\"].plot(ax=ax, style='.-')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig(save_to)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Compute metric for 1.0_1.0\n",
      "[+] Compute metric for 2.0_2.0\n",
      "[+] Compute metric for 3.0_3.0\n",
      "[+] Compute metric for 4.0_4.0\n",
      "[+] Compute metric for 5.0_5.0\n",
      "[+] Compute metric for 6.0_6.0\n",
      "[+] Compute metric for 7.0_7.0\n",
      "[+] Compute metric for 8.0_8.0\n",
      "[+] Compute metric for 9.0_9.0\n",
      "[+] Compute metric for 10.0_10.0\n"
     ]
    }
   ],
   "source": [
    "df_metric_noise, df_metric_amplitude, df_metric_time_shift = compute_metrics(dsets_real, dsets_fake, STYLE_NAMES, SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(df_metric_amplitude, \"[Style Time Bench]: Metric on Amplitude\", f\"{SAVE_FOLDER}/STB_\")\n",
    "plot_metric(df_metric_noise, \"[Style Time Bench]: Metric on Noise\", SAVE_FOLDER)\n",
    "plot_metric(df_metric_time_shift, \"[Style Time Bench]: Metric on Time Shift\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval import tstr as tstr_utils\n",
    "\n",
    "def tstr(\n",
    "    dset_train_real,\n",
    "    dset_valid_real,\n",
    "    dset_train_fake, \n",
    "    dset_valid_fake, \n",
    "    save_to:str):\n",
    "\n",
    "    print('[+] Train Real, Test Real.')\n",
    "    real_performances, hist_real = tstr_utils.train_naive_discriminator(dset_train_real, dset_valid_real, SEQ_SHAPE, epochs=5, n_classes=5)\n",
    "\n",
    "    print(\"[+] Train Synthetic, Test Synthetic\")\n",
    "    gen_perf1, hist_fake1 = tstr_utils.train_naive_discriminator(dset_train_fake, dset_valid_fake, SEQ_SHAPE, epochs=5, n_classes=5)\n",
    "    \n",
    "    print(\"[+] Train Synthetic, Test Real\")\n",
    "    gen_perf2, hist_fake2 = tstr_utils.train_naive_discriminator(dset_train_fake, dset_valid_real, SEQ_SHAPE, epochs=5, n_classes=5)\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    ax = plt.subplot(211)\n",
    "    \n",
    "    plt.plot(hist_real.history[\"loss\"], \".-\", label='Train Real Test Real (Train)')\n",
    "    plt.plot(hist_real.history[\"val_loss\"], \".-\", label='Train Real Test Real (Valid)')\n",
    "    \n",
    "    plt.plot(hist_fake1.history[\"loss\"], \".-\", label='Train Synthetic, Test Synthetic (Train)')\n",
    "    plt.plot(hist_fake1.history[\"val_loss\"], \".-\", label='Train Synthetic, Test Synthetic (Valid)')\n",
    "    \n",
    "    plt.plot(hist_fake2.history[\"loss\"], \".-\", label='Train Real, Test Synthetic (Train)')\n",
    "    plt.plot(hist_fake2.history[\"val_loss\"], \".-\", label='Train Real, Test Synthetic (Valid)')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax = plt.subplot(212)\n",
    "    \n",
    "    plt.plot(hist_real.history[\"sparse_categorical_accuracy\"], \".-\", label='Classification Acc on Real (Train)')\n",
    "    plt.plot(hist_real.history[\"val_sparse_categorical_accuracy\"], \".-\", label='Classification Acc on Real (Valid)')\n",
    "    \n",
    "    plt.plot(hist_fake1.history[\"sparse_categorical_accuracy\"], \".-\", label='Train Synthetic, Test Synthetic (Train)')\n",
    "    plt.plot(hist_fake1.history[\"val_sparse_categorical_accuracy\"], \".-\", label='Train Synthetic, Test Synthetic (Valid)')\n",
    "    \n",
    "    plt.plot(hist_fake2.history[\"sparse_categorical_accuracy\"], \".-\", label='Train Real, Test Synthetic (Train)')\n",
    "    plt.plot(hist_fake2.history[\"val_sparse_categorical_accuracy\"], \".-\", label='Train Real, Test Synthetic (Valid)')\n",
    "    \n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig(save_to)\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return real_performances, gen_perf2\n",
    "\n",
    "\n",
    "def tstr_on_styles(real_dataset, fake_dataset, style_names, model_folder):\n",
    "    tstr_stats = {}\n",
    "\n",
    "    for style_ in style_names:\n",
    "        print(f'[+] Training on dataset {style_}.')\n",
    "        \n",
    "        perf_on_real, perf_on_fake = tstr(\n",
    "            real_dataset[f\"{style_}_train\"],\n",
    "            real_dataset[f\"{style_}_valid\"],\n",
    "            fake_dataset[f\"{style_}_train\"],\n",
    "            fake_dataset[f\"{style_}_valid\"], \n",
    "            f'{model_folder}/tstr_{style_}.png'\n",
    "        )\n",
    "        \n",
    "        tstr_stats[f\"{style_}_real\"] = [perf_on_real]\n",
    "        tstr_stats[f\"{style_}_gen\"] = [perf_on_fake]\n",
    "        \n",
    "    tstr_stats = pd.DataFrame.from_dict(tstr_stats)\n",
    "\n",
    "    tstr_stats.to_hdf(f\"{model_folder}/tstr.h5\", key=\"data\")\n",
    "    \n",
    "    return tstr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training on dataset 1.0_1.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0817 - sparse_categorical_accuracy: 0.9711 - val_loss: 0.0264 - val_sparse_categorical_accuracy: 0.9904\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9951\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.0114 - val_sparse_categorical_accuracy: 0.9970\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0051 - val_sparse_categorical_accuracy: 0.9982\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.0144 - val_sparse_categorical_accuracy: 0.9953\n",
      "1041/1041 [==============================] - 10s 9ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9953\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 4.3847 - sparse_categorical_accuracy: 0.1333 - val_loss: 2.3768 - val_sparse_categorical_accuracy: 0.4000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7831 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.5675 - val_sparse_categorical_accuracy: 0.3000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.7343 - sparse_categorical_accuracy: 0.2000 - val_loss: 1.0444 - val_sparse_categorical_accuracy: 0.6333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.0180 - sparse_categorical_accuracy: 0.6333 - val_loss: 1.0389 - val_sparse_categorical_accuracy: 0.6333\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9262 - sparse_categorical_accuracy: 0.6333 - val_loss: 1.3361 - val_sparse_categorical_accuracy: 0.6333\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3361 - sparse_categorical_accuracy: 0.6333\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 9.5710 - sparse_categorical_accuracy: 0.1000 - val_loss: 3.9911 - val_sparse_categorical_accuracy: 0.0566\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 4.9600 - sparse_categorical_accuracy: 0.0667 - val_loss: 3.2551 - val_sparse_categorical_accuracy: 0.1377\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 3.2896 - sparse_categorical_accuracy: 0.1333 - val_loss: 2.9262 - val_sparse_categorical_accuracy: 0.3059\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.2504 - sparse_categorical_accuracy: 0.4333 - val_loss: 4.2001 - val_sparse_categorical_accuracy: 0.2952\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 3.1662 - sparse_categorical_accuracy: 0.4333 - val_loss: 4.2377 - val_sparse_categorical_accuracy: 0.2952\n",
      "1041/1041 [==============================] - 10s 9ms/step - loss: 4.2377 - sparse_categorical_accuracy: 0.2952\n",
      "[+] Training on dataset 2.0_2.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9768 - val_loss: 0.0198 - val_sparse_categorical_accuracy: 0.9941\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.0013 - val_sparse_categorical_accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 57s 14ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9982 - val_loss: 8.3998e-04 - val_sparse_categorical_accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 57s 14ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9986 - val_loss: 4.1354e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 57s 14ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0066 - val_sparse_categorical_accuracy: 0.9987\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9987\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.5267 - sparse_categorical_accuracy: 0.1333 - val_loss: 4.6773 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.4371 - sparse_categorical_accuracy: 0.1667 - val_loss: 2.5608 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5957 - sparse_categorical_accuracy: 0.0667 - val_loss: 2.1940 - val_sparse_categorical_accuracy: 0.0333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.2330 - sparse_categorical_accuracy: 0.0667 - val_loss: 2.3008 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2013 - sparse_categorical_accuracy: 0.2667 - val_loss: 2.5260 - val_sparse_categorical_accuracy: 0.3000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5260 - sparse_categorical_accuracy: 0.3000\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 8.9135 - sparse_categorical_accuracy: 0.1667 - val_loss: 5.7397 - val_sparse_categorical_accuracy: 0.1487\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 5.1155 - sparse_categorical_accuracy: 0.1333 - val_loss: 3.3150 - val_sparse_categorical_accuracy: 0.3179\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.4646 - sparse_categorical_accuracy: 0.5333 - val_loss: 3.6279 - val_sparse_categorical_accuracy: 0.2952\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.8298 - sparse_categorical_accuracy: 0.4333 - val_loss: 2.9822 - val_sparse_categorical_accuracy: 0.3010\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.4131 - sparse_categorical_accuracy: 0.4333 - val_loss: 2.2915 - val_sparse_categorical_accuracy: 0.5644\n",
      "1041/1041 [==============================] - 10s 9ms/step - loss: 2.2915 - sparse_categorical_accuracy: 0.5644\n",
      "[+] Training on dataset 3.0_3.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0637 - sparse_categorical_accuracy: 0.9793 - val_loss: 0.0050 - val_sparse_categorical_accuracy: 0.9987\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0012 - val_sparse_categorical_accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9987 - val_loss: 4.0499e-04 - val_sparse_categorical_accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9990 - val_loss: 7.0741e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.0013 - val_sparse_categorical_accuracy: 0.9995\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 0.0013 - sparse_categorical_accuracy: 0.9995\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 784ms/step - loss: 1.8220 - sparse_categorical_accuracy: 0.3000 - val_loss: 1.3918 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4541 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.2067 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2273 - sparse_categorical_accuracy: 0.6333 - val_loss: 1.0704 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0402 - sparse_categorical_accuracy: 0.6333 - val_loss: 0.9799 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.8990 - sparse_categorical_accuracy: 0.6333 - val_loss: 0.9346 - val_sparse_categorical_accuracy: 0.7667\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.9346 - sparse_categorical_accuracy: 0.7667\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 6.7624 - sparse_categorical_accuracy: 0.1333 - val_loss: 4.7610 - val_sparse_categorical_accuracy: 0.0780\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 5.0578 - sparse_categorical_accuracy: 0.0333 - val_loss: 3.4018 - val_sparse_categorical_accuracy: 0.1089\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 3.4590 - sparse_categorical_accuracy: 0.0333 - val_loss: 2.5193 - val_sparse_categorical_accuracy: 0.2545\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.3350 - sparse_categorical_accuracy: 0.3333 - val_loss: 2.6086 - val_sparse_categorical_accuracy: 0.2995\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.2401 - sparse_categorical_accuracy: 0.4333 - val_loss: 2.7683 - val_sparse_categorical_accuracy: 0.2952\n",
      "1041/1041 [==============================] - 10s 9ms/step - loss: 2.7683 - sparse_categorical_accuracy: 0.2952\n",
      "[+] Training on dataset 4.0_4.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0440 - sparse_categorical_accuracy: 0.9895 - val_loss: 6.0796e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9986 - val_loss: 1.8091e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9994 - val_loss: 1.1155e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0014 - sparse_categorical_accuracy: 0.9996 - val_loss: 4.0787e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0021 - sparse_categorical_accuracy: 0.9994 - val_loss: 3.5440e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 3.5440e-05 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 689ms/step - loss: 2.6505 - sparse_categorical_accuracy: 0.2667 - val_loss: 1.3186 - val_sparse_categorical_accuracy: 0.4667\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.4222 - sparse_categorical_accuracy: 0.5667 - val_loss: 1.9645 - val_sparse_categorical_accuracy: 0.3667\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.7577 - sparse_categorical_accuracy: 0.4667 - val_loss: 2.1558 - val_sparse_categorical_accuracy: 0.3333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.8251 - sparse_categorical_accuracy: 0.4333 - val_loss: 2.0035 - val_sparse_categorical_accuracy: 0.3667\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.6953 - sparse_categorical_accuracy: 0.4667 - val_loss: 1.6761 - val_sparse_categorical_accuracy: 0.4333\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.6761 - sparse_categorical_accuracy: 0.4333\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 2.8123 - sparse_categorical_accuracy: 0.2333 - val_loss: 2.6596 - val_sparse_categorical_accuracy: 0.1751\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.0486 - sparse_categorical_accuracy: 0.4000 - val_loss: 2.2668 - val_sparse_categorical_accuracy: 0.2453\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.7672 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.9373 - val_sparse_categorical_accuracy: 0.2100\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.6204 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.8161 - val_sparse_categorical_accuracy: 0.3862\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.6239 - sparse_categorical_accuracy: 0.3000 - val_loss: 1.7243 - val_sparse_categorical_accuracy: 0.4555\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 1.7243 - sparse_categorical_accuracy: 0.4555\n",
      "[+] Training on dataset 5.0_5.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9844 - val_loss: 6.5064e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 54s 13ms/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9990 - val_loss: 8.5434e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 54s 13ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0280 - val_sparse_categorical_accuracy: 0.9940\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9994 - val_loss: 4.8275e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 3.3470e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5376e-06 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 6.5376e-06 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1724 - sparse_categorical_accuracy: 0.2000 - val_loss: 2.6646 - val_sparse_categorical_accuracy: 0.1333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6883 - sparse_categorical_accuracy: 0.0667 - val_loss: 2.8241 - val_sparse_categorical_accuracy: 0.2000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6174 - sparse_categorical_accuracy: 0.3000 - val_loss: 2.5930 - val_sparse_categorical_accuracy: 0.3333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.2996 - sparse_categorical_accuracy: 0.4333 - val_loss: 2.1004 - val_sparse_categorical_accuracy: 0.4000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.8024 - sparse_categorical_accuracy: 0.5333 - val_loss: 1.6846 - val_sparse_categorical_accuracy: 0.4000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.6846 - sparse_categorical_accuracy: 0.4000\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.7241 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.6437 - val_sparse_categorical_accuracy: 0.3608\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.3591 - sparse_categorical_accuracy: 0.4667 - val_loss: 1.3625 - val_sparse_categorical_accuracy: 0.4659\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.0983 - sparse_categorical_accuracy: 0.5333 - val_loss: 1.1879 - val_sparse_categorical_accuracy: 0.5922\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.9469 - sparse_categorical_accuracy: 0.6000 - val_loss: 1.0932 - val_sparse_categorical_accuracy: 0.6199\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.8258 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.0115 - val_sparse_categorical_accuracy: 0.6478\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 1.0115 - sparse_categorical_accuracy: 0.6478\n",
      "[+] Training on dataset 6.0_6.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 57s 13ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9871 - val_loss: 5.4696e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9992 - val_loss: 9.7503e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 57s 14ms/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9991 - val_loss: 4.7848e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 58s 14ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9996 - val_loss: 3.0902e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 57s 14ms/step - loss: 1.6778e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2037e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 1.2037e-04 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 707ms/step - loss: 12.9325 - sparse_categorical_accuracy: 0.1000 - val_loss: 7.6750 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.6802 - sparse_categorical_accuracy: 0.1333 - val_loss: 5.0948 - val_sparse_categorical_accuracy: 0.1333\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.8583 - sparse_categorical_accuracy: 0.1333 - val_loss: 2.7760 - val_sparse_categorical_accuracy: 0.2667\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.4970 - sparse_categorical_accuracy: 0.3333 - val_loss: 2.9919 - val_sparse_categorical_accuracy: 0.3333\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6942 - sparse_categorical_accuracy: 0.4000 - val_loss: 3.4812 - val_sparse_categorical_accuracy: 0.3667\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4812 - sparse_categorical_accuracy: 0.3667\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 14.0987 - sparse_categorical_accuracy: 0.1000 - val_loss: 7.6811 - val_sparse_categorical_accuracy: 0.1398\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 7.7308 - sparse_categorical_accuracy: 0.1000 - val_loss: 3.5197 - val_sparse_categorical_accuracy: 0.1944\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 3.4173 - sparse_categorical_accuracy: 0.2667 - val_loss: 3.1292 - val_sparse_categorical_accuracy: 0.2720\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.7286 - sparse_categorical_accuracy: 0.2000 - val_loss: 3.4879 - val_sparse_categorical_accuracy: 0.3499\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.8625 - sparse_categorical_accuracy: 0.3667 - val_loss: 3.6809 - val_sparse_categorical_accuracy: 0.3561\n",
      "1041/1041 [==============================] - 11s 10ms/step - loss: 3.6809 - sparse_categorical_accuracy: 0.3561\n",
      "[+] Training on dataset 7.0_7.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 59s 14ms/step - loss: 0.0381 - sparse_categorical_accuracy: 0.9890 - val_loss: 1.6136e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 59s 14ms/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9993 - val_loss: 5.5296e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 59s 14ms/step - loss: 2.7561e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.2287e-06 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 56s 13ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9995 - val_loss: 2.6674e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 59s 14ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9995 - val_loss: 1.2165e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 1.2165e-04 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 728ms/step - loss: 5.6986 - sparse_categorical_accuracy: 0.2667 - val_loss: 4.1491 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.6621 - sparse_categorical_accuracy: 0.1667 - val_loss: 2.9417 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.1922 - sparse_categorical_accuracy: 0.1667 - val_loss: 2.6226 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5449 - sparse_categorical_accuracy: 0.4000 - val_loss: 2.4144 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0821 - sparse_categorical_accuracy: 0.5000 - val_loss: 2.0816 - val_sparse_categorical_accuracy: 0.3000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0816 - sparse_categorical_accuracy: 0.3000\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 8.9398 - sparse_categorical_accuracy: 0.1333 - val_loss: 4.4838 - val_sparse_categorical_accuracy: 0.1841\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 4.0936 - sparse_categorical_accuracy: 0.3000 - val_loss: 2.8180 - val_sparse_categorical_accuracy: 0.3505\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.0801 - sparse_categorical_accuracy: 0.4667 - val_loss: 2.5938 - val_sparse_categorical_accuracy: 0.4312\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.9501 - sparse_categorical_accuracy: 0.5333 - val_loss: 2.5173 - val_sparse_categorical_accuracy: 0.5077\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 2.1375 - sparse_categorical_accuracy: 0.5000 - val_loss: 2.5249 - val_sparse_categorical_accuracy: 0.5394\n",
      "1041/1041 [==============================] - 10s 10ms/step - loss: 2.5249 - sparse_categorical_accuracy: 0.5394\n",
      "[+] Training on dataset 8.0_8.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 55s 13ms/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.0016 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 71s 17ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9994 - val_loss: 9.6723e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 76s 18ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9994 - val_loss: 9.4728e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 73s 18ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - val_loss: 7.2828e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 77s 18ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - val_loss: 1.2467e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 15s 14ms/step - loss: 1.2467e-04 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 5.1121 - sparse_categorical_accuracy: 0.2333 - val_loss: 3.0170 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 3.2917 - sparse_categorical_accuracy: 0.2000 - val_loss: 2.3054 - val_sparse_categorical_accuracy: 0.3333\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 2.1717 - sparse_categorical_accuracy: 0.3000 - val_loss: 2.0963 - val_sparse_categorical_accuracy: 0.4333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 1.7122 - sparse_categorical_accuracy: 0.5333 - val_loss: 2.2938 - val_sparse_categorical_accuracy: 0.3667\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 1.8103 - sparse_categorical_accuracy: 0.4667 - val_loss: 2.4659 - val_sparse_categorical_accuracy: 0.4000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.4659 - sparse_categorical_accuracy: 0.4000\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 4.5404 - sparse_categorical_accuracy: 0.4333 - val_loss: 3.0700 - val_sparse_categorical_accuracy: 0.3187\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 3.1013 - sparse_categorical_accuracy: 0.3000 - val_loss: 2.6904 - val_sparse_categorical_accuracy: 0.3194\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 3.0999 - sparse_categorical_accuracy: 0.2333 - val_loss: 2.5011 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 2.8320 - sparse_categorical_accuracy: 0.2000 - val_loss: 2.3710 - val_sparse_categorical_accuracy: 0.2626\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 2.4330 - sparse_categorical_accuracy: 0.3333 - val_loss: 2.2465 - val_sparse_categorical_accuracy: 0.2620\n",
      "1041/1041 [==============================] - 14s 13ms/step - loss: 2.2465 - sparse_categorical_accuracy: 0.2620\n",
      "[+] Training on dataset 9.0_9.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 77s 18ms/step - loss: 0.0396 - sparse_categorical_accuracy: 0.9878 - val_loss: 0.0070 - val_sparse_categorical_accuracy: 0.9964\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 76s 18ms/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9992 - val_loss: 5.6340e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 77s 18ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9994 - val_loss: 5.2174e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 76s 18ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9994 - val_loss: 1.1162e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 76s 18ms/step - loss: 4.1959e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3101e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 15s 14ms/step - loss: 1.3101e-05 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 7.5302 - sparse_categorical_accuracy: 0.3333 - val_loss: 4.7549 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 3.8650 - sparse_categorical_accuracy: 0.4000 - val_loss: 4.7296 - val_sparse_categorical_accuracy: 0.3000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 4.5632 - sparse_categorical_accuracy: 0.2000 - val_loss: 3.2999 - val_sparse_categorical_accuracy: 0.2000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 2.9154 - sparse_categorical_accuracy: 0.1667 - val_loss: 3.7666 - val_sparse_categorical_accuracy: 0.2000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 2.7310 - sparse_categorical_accuracy: 0.2333 - val_loss: 4.5047 - val_sparse_categorical_accuracy: 0.3000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 4.5047 - sparse_categorical_accuracy: 0.3000\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 10.4857 - sparse_categorical_accuracy: 0.4333 - val_loss: 7.5413 - val_sparse_categorical_accuracy: 0.3096\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 8.2992 - sparse_categorical_accuracy: 0.2667 - val_loss: 5.7142 - val_sparse_categorical_accuracy: 0.2931\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 6.6167 - sparse_categorical_accuracy: 0.3000 - val_loss: 4.4184 - val_sparse_categorical_accuracy: 0.2552\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 4.7021 - sparse_categorical_accuracy: 0.1667 - val_loss: 4.8321 - val_sparse_categorical_accuracy: 0.1917\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 4.5540 - sparse_categorical_accuracy: 0.1333 - val_loss: 5.0580 - val_sparse_categorical_accuracy: 0.2121\n",
      "1041/1041 [==============================] - 14s 13ms/step - loss: 5.0580 - sparse_categorical_accuracy: 0.2121\n",
      "[+] Training on dataset 10.0_10.0.\n",
      "[+] Train Real, Test Real.\n",
      "Epoch 1/5\n",
      "4166/4166 [==============================] - 77s 18ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9906 - val_loss: 1.4232e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4166/4166 [==============================] - 76s 18ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9994 - val_loss: 7.7207e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4166/4166 [==============================] - 77s 18ms/step - loss: 1.4810e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9489e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4166/4166 [==============================] - 75s 18ms/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9995 - val_loss: 8.4420e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4166/4166 [==============================] - 77s 18ms/step - loss: 3.1381e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.7312e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "1041/1041 [==============================] - 14s 14ms/step - loss: 4.7312e-05 - sparse_categorical_accuracy: 1.0000\n",
      "[+] Train Synthetic, Test Synthetic\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 12.2123 - sparse_categorical_accuracy: 0.1333 - val_loss: 8.4357 - val_sparse_categorical_accuracy: 0.1333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 8.2647 - sparse_categorical_accuracy: 0.1667 - val_loss: 5.9544 - val_sparse_categorical_accuracy: 0.1333\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 5.6920 - sparse_categorical_accuracy: 0.1000 - val_loss: 4.6526 - val_sparse_categorical_accuracy: 0.2000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 4.3482 - sparse_categorical_accuracy: 0.1000 - val_loss: 3.5384 - val_sparse_categorical_accuracy: 0.3000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 3.1622 - sparse_categorical_accuracy: 0.2333 - val_loss: 2.9550 - val_sparse_categorical_accuracy: 0.4333\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.9550 - sparse_categorical_accuracy: 0.4333\n",
      "[+] Train Synthetic, Test Real\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 16s 16s/step - loss: 2.8745 - sparse_categorical_accuracy: 0.1333 - val_loss: 2.8132 - val_sparse_categorical_accuracy: 0.2002\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.2747 - sparse_categorical_accuracy: 0.1667 - val_loss: 2.3752 - val_sparse_categorical_accuracy: 0.2356\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.8039 - sparse_categorical_accuracy: 0.3333 - val_loss: 2.0812 - val_sparse_categorical_accuracy: 0.2832\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.4352 - sparse_categorical_accuracy: 0.4667 - val_loss: 1.9012 - val_sparse_categorical_accuracy: 0.3376\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2133 - sparse_categorical_accuracy: 0.5667 - val_loss: 1.7836 - val_sparse_categorical_accuracy: 0.3676\n",
      "1041/1041 [==============================] - 14s 14ms/step - loss: 1.7836 - sparse_categorical_accuracy: 0.3676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.0_1.0_real</th>\n",
       "      <th>1.0_1.0_gen</th>\n",
       "      <th>2.0_2.0_real</th>\n",
       "      <th>2.0_2.0_gen</th>\n",
       "      <th>3.0_3.0_real</th>\n",
       "      <th>3.0_3.0_gen</th>\n",
       "      <th>4.0_4.0_real</th>\n",
       "      <th>4.0_4.0_gen</th>\n",
       "      <th>5.0_5.0_real</th>\n",
       "      <th>5.0_5.0_gen</th>\n",
       "      <th>6.0_6.0_real</th>\n",
       "      <th>6.0_6.0_gen</th>\n",
       "      <th>7.0_7.0_real</th>\n",
       "      <th>7.0_7.0_gen</th>\n",
       "      <th>8.0_8.0_real</th>\n",
       "      <th>8.0_8.0_gen</th>\n",
       "      <th>9.0_9.0_real</th>\n",
       "      <th>9.0_9.0_gen</th>\n",
       "      <th>10.0_10.0_real</th>\n",
       "      <th>10.0_10.0_gen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.295179</td>\n",
       "      <td>0.998739</td>\n",
       "      <td>0.564406</td>\n",
       "      <td>0.999505</td>\n",
       "      <td>0.295179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.64777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.262023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.212071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.36757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1.0_1.0_real  1.0_1.0_gen  2.0_2.0_real  2.0_2.0_gen  3.0_3.0_real  \\\n",
       "0      0.995287     0.295179      0.998739     0.564406      0.999505   \n",
       "\n",
       "   3.0_3.0_gen  4.0_4.0_real  4.0_4.0_gen  5.0_5.0_real  5.0_5.0_gen  \\\n",
       "0     0.295179           1.0     0.455512           1.0      0.64777   \n",
       "\n",
       "   6.0_6.0_real  6.0_6.0_gen  7.0_7.0_real  7.0_7.0_gen  8.0_8.0_real  \\\n",
       "0           1.0     0.356133           1.0       0.5394           1.0   \n",
       "\n",
       "   8.0_8.0_gen  9.0_9.0_real  9.0_9.0_gen  10.0_10.0_real  10.0_10.0_gen  \n",
       "0     0.262023           1.0     0.212071             1.0        0.36757  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstr_on_styles(dsets_real, dsets_fake, STYLE_NAMES, SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def multi_umap_plot(real_styles, gen_styles):\n",
    "    (_, _, seq_len, n_sigs) = real_styles.shape\n",
    "    \n",
    "    print(real_styles.shape)\n",
    "    print(fake_batches.shape)\n",
    "    \n",
    "    concatenated = tf.concat((real_styles, gen_styles), 0)\n",
    "\n",
    "    concatenated = tf.reshape(concatenated, (-1, seq_len, n_sigs))\n",
    "    concatenated = tf.transpose(concatenated, (0, 2, 1))\n",
    "    \n",
    "    concatenated = tf.reshape(concatenated, (concatenated.shape[0], -1))\n",
    "\n",
    "    # # # Normalize all sequences for the reducer.\n",
    "    _mean, _std = tf.math.reduce_mean(concatenated), tf.math.reduce_std(concatenated)\n",
    "    concatenated = (concatenated - _mean)/_std\n",
    "\n",
    "    reducer = umap.UMAP(n_neighbors=300, min_dist=1., random_state=42, metric=\"euclidean\") \n",
    "    reduced = reducer.fit_transform(concatenated)\n",
    "    return reduced\n",
    "\n",
    "\n",
    "def multi_tsne_plot(real_styles, gen_styles):\n",
    "    (_, _, seq_len, n_sigs) = real_styles.shape\n",
    "    \n",
    "    concatenated = tf.concat((real_styles, gen_styles), 0)\n",
    "\n",
    "    concatenated = tf.reshape(concatenated, (-1, seq_len, n_sigs))\n",
    "    concatenated = tf.transpose(concatenated, (0, 2, 1))\n",
    "    \n",
    "    concatenated = tf.reshape(concatenated, (concatenated.shape[0], -1))\n",
    "\n",
    "    # # # Normalize all sequences for the reducer.\n",
    "    _mean, _std = tf.math.reduce_mean(concatenated), tf.math.reduce_std(concatenated)\n",
    "    concatenated = (concatenated - _mean)/_std\n",
    "\n",
    "    reducer = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=150, random_state=42)\n",
    "    reduced = reducer.fit_transform(concatenated)\n",
    "    return reduced\n",
    "\n",
    "\n",
    "def generate_per_style_batch(dset_real, dset_fake, style_names):\n",
    "    real_batches = []\n",
    "    fake_batches = []\n",
    "\n",
    "    for _, style_ in enumerate(style_names):\n",
    "        real_style_batch = get_batches(dset_real[f\"{style_}_valid\"], 10)\n",
    "        fake_style_batch = get_batches(dset_fake[f\"{style_}_valid\"], 10)\n",
    "        \n",
    "        real_batches.append(real_style_batch)\n",
    "        fake_batches.append(fake_style_batch)\n",
    "        \n",
    "    return np.array(real_batches), np.array(fake_batches) \n",
    "\n",
    "def dimentionality_reduction_plot(real_batches, fake_batches, style_names, model_folder, type=\"umap\"):\n",
    "    \n",
    "    if type == 'umap':\n",
    "        reduced_points = multi_umap_plot(real_batches, fake_batches)\n",
    "    elif type == \"tsne\":\n",
    "        reduced_points = multi_tsne_plot(real_batches, fake_batches)\n",
    "    else: \n",
    "        raise Exception(\"No Dimentionality reduction algorthm selected.\")\n",
    "        \n",
    "    n_styles = len(style_names)\n",
    "    (n_styles, bs, _, _) = real_batches.shape\n",
    "\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    colors = cmap(np.linspace(0, 1, n_styles*2))\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    for i in range(n_styles):\n",
    "        ri, ro = i*bs, (i+1)*bs\n",
    "        fi, fo =  (i+ n_styles) * bs, (i+ n_styles+ 1) * bs\n",
    "        \n",
    "        plt.scatter(reduced_points[ri:ro, 0], reduced_points[ri:ro, 1], label=f\"Real Style {i+ 1}\", alpha=0.5, color=colors[2*i], s=4)\n",
    "        plt.scatter(reduced_points[fi:fo, 0], reduced_points[fi:fo, 1], label=f\"Generated Style {i+ 1}\", alpha=0.5, color=colors[2*i+1 ], s=4)\n",
    "        \n",
    "    plt.grid()\n",
    "    plt.title(f\"{type} Reduction of Time Series\", fontsize=15)\n",
    "    plt.ylabel(f\"y_{type}\", fontsize=15)\n",
    "    plt.xlabel(f\"x_{type}\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{model_folder}/{type}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batches, fake_batches = generate_per_style_batch(dsets_real, dsets_fake, STYLE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 640, 64, 7)\n",
      "(10, 30, 64, 7)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [10,640,64,7] vs. shape[1] = [10,30,64,7] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdimentionality_reduction_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTYLE_NAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mumap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dimentionality_reduction_plot(real_batches, fake_batches, STYLE_NAMES, SAVE_FOLDER, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsne\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 61\u001b[0m, in \u001b[0;36mdimentionality_reduction_plot\u001b[0;34m(real_batches, fake_batches, style_names, model_folder, type)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdimentionality_reduction_plot\u001b[39m(real_batches, fake_batches, style_names, model_folder, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mumap\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mumap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m         reduced_points \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_umap_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsne\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     63\u001b[0m         reduced_points \u001b[38;5;241m=\u001b[39m multi_tsne_plot(real_batches, fake_batches)\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36mmulti_umap_plot\u001b[0;34m(real_styles, gen_styles)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(real_styles\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(fake_batches\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_styles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_styles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(concatenated, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_len, n_sigs))\n\u001b[1;32m     13\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(concatenated, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [10,640,64,7] vs. shape[1] = [10,30,64,7] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "dimentionality_reduction_plot(real_batches, fake_batches, STYLE_NAMES, SAVE_FOLDER, \"umap\")\n",
    "dimentionality_reduction_plot(real_batches, fake_batches, STYLE_NAMES, SAVE_FOLDER, \"tsne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
