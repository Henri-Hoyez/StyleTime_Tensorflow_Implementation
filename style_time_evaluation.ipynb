{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Time Evaluation.\n",
    "This script will evaluate Style Time As the Paper do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(seq_shape:tuple) -> tf.keras.Model:\n",
    "    \"\"\"Make the LSTM Model as stated in the paper (2x100 lstm layers.)\n",
    "\n",
    "    Args:\n",
    "        seq_shape (tuple): The Shape of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    print(seq_shape)\n",
    "    model.add(tf.keras.layers.Input(seq_shape))\n",
    "    model.add(tf.keras.layers.Reshape((seq_shape[-1], seq_shape[-2])))\n",
    "    \n",
    "    model.add(tf.keras.layers.LSTM(100, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(100, return_sequences=True))\n",
    "    \n",
    "    model.add(\n",
    "        tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(seq_shape[0])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(tf.keras.layers.Reshape(seq_shape))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='Adam', \n",
    "        loss=\"mae\", \n",
    "        metrics=tf.keras.metrics.MeanAbsoluteError()\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.plot(hist.history['loss'], '.-', label=\" Train Loss\")\n",
    "    plt.plot(hist.history['val_loss'], '.-', label=\"Valid Loss\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(train_dset:np.ndarray, valid_dset:np.ndarray, epochs=50) -> tf.keras.Model:\n",
    "    model = lstm_model(train_dset.shape[1:])\n",
    "    \n",
    "    hist = model.fit(train_dset, train_dset, validation_data=(valid_dset, valid_dset), epochs=epochs)\n",
    "    \n",
    "    plot_learning_curves(hist)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model:tf.keras.Model, test_dataset:np.ndarray)-> float:\n",
    "    reconstructions = model(test_dataset)\n",
    "    \n",
    "    return np.mean((np.sum(np.abs(reconstructions - test_dataset), axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE STOCKS RESULTS.\n",
    "### TSTR MAE on \"In Sample\" Dataset.\n",
    "\n",
    "In this part, we train the model on the \"style transfered\" data. Then, we evaluate it on the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_train = np.load(\"data/google_stocks/preprocessed/style_train.npy\")\n",
    "in_sample_test = np.load(\"data/google_stocks/preprocessed/style_test.npy\")\n",
    "\n",
    "transfered_in_samples_train = np.load(\"data/google_stocks/preprocessed/normal_content_train_transfered.npy\")\n",
    "transfered_in_samples_test = np.load(\"data/google_stocks/preprocessed/normal_content_test_transfered.npy\")\n",
    "\n",
    "# in_sample_trained_model = train_model(transfered_in_samples_train, in_sample_test, epochs=100)\n",
    "in_sample_trained_model = train_model(transfered_in_samples_train, transfered_in_samples_test, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot some sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(model, sequence, title=\"\"):\n",
    "    model_reconstruction = model(np.array([sequence]))[0]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_title(title)\n",
    "    ax.plot(sequence[0], \".-\", label='model True')\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.plot(model_reconstruction[0], \".-\", label='model Reconstruction')\n",
    "\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_reconstruction(\n",
    "    in_sample_trained_model, \n",
    "    in_sample_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $REAL$ $SEQUENCE$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(\n",
    "    in_sample_trained_model, \n",
    "    transfered_in_samples_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $FAKE$ $SEQUENCE$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model \n",
    "With the Metric paper's metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_model_real_mae = evaluate_model(in_sample_trained_model, in_sample_test)\n",
    "in_sample_model_fake_mae = evaluate_model(in_sample_trained_model, transfered_in_samples_test)\n",
    "\n",
    "in_sample_model_real_mae, in_sample_model_fake_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on \"Perturbed\" Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_generated_train = np.load(\"data/google_stocks/preprocessed/perturbed_content_train_transfered.npy\")\n",
    "perturbed_generated_test = np.load(\"data/google_stocks/preprocessed/perturbed_content_test_transfered.npy\")\n",
    "\n",
    "perturbed_trained_model = train_model(perturbed_generated_train, perturbed_generated_test, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(\n",
    "    perturbed_trained_model, \n",
    "    in_sample_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $REAL$ $SEQUENCE$\")\n",
    "\n",
    "plot_reconstruction(\n",
    "    perturbed_trained_model, \n",
    "    perturbed_generated_test[1],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $FAKE$ $SEQUENCE$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_model_real_mae = evaluate_model(perturbed_trained_model, in_sample_test)\n",
    "perturbed_model_fake_mae = evaluate_model(perturbed_trained_model, perturbed_generated_test)\n",
    "\n",
    "perturbed_model_real_mae, perturbed_model_fake_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a DataFrame with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array([[in_sample_model_real_mae],  [perturbed_model_real_mae]])\n",
    "# values = 1- values\n",
    "cols = [\"Google Stocks TSTR MAE\"]\n",
    "index = [\"Style Time (In Samples)\", \"Style Time (Perturbed)\"]\n",
    "\n",
    "google_stocks_results = pd.DataFrame(index=index, columns=cols, data=values)\n",
    "\n",
    "google_stocks_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Dataset\n",
    "### Again, TSTR MAE on \"In Sample\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_real_train = np.load(\"data/energy/preprocessed/style_train.npy\")\n",
    "energy_real_test = np.load(\"data/energy/preprocessed/style_test.npy\")\n",
    "\n",
    "energy_is_train = np.load(\"data/energy/preprocessed/in_sample_train.npy\")\n",
    "energy_is_test = np.load(\"data/energy/preprocessed/in_sample_train.npy\")\n",
    "\n",
    "energy_is_model = train_model(energy_is_train, energy_is_test, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(\n",
    "    energy_is_model, \n",
    "    energy_real_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $REAL$ $SEQUENCE$\"\n",
    "    )\n",
    "\n",
    "plot_reconstruction(\n",
    "    energy_is_model, \n",
    "    energy_is_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $FAKE$ $SEQUENCE$\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSTR MAE on \"perturbed\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_pert_train = np.load(\"data/energy/preprocessed/perturbed_train.npy\")\n",
    "energy_pert_test = np.load(\"data/energy/preprocessed/perturbed_test.npy\")\n",
    "\n",
    "energy_pert_model = train_model(energy_pert_train, energy_pert_test, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(\n",
    "    energy_pert_model, \n",
    "    energy_real_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $REAL$ $SEQUENCE$\"\n",
    "    )\n",
    "\n",
    "plot_reconstruction(\n",
    "    energy_pert_model, \n",
    "    energy_pert_test[0],\n",
    "    \"Comparison between the model reconstruction and the ground truth.\\n Here on a $FAKE$ $SEQUENCE$\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_is_res_on_real = evaluate_model(energy_is_model, energy_real_test)\n",
    "energy_is_res_on_fake = evaluate_model(energy_is_model, energy_is_test)\n",
    "\n",
    "energy_is_res_on_real, energy_is_res_on_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_pert_res_on_real = evaluate_model(energy_pert_model, energy_real_test)\n",
    "energy_pert_res_on_fake = evaluate_model(energy_pert_model, energy_pert_test)\n",
    "\n",
    "energy_pert_res_on_real, energy_pert_res_on_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array([[energy_is_res_on_real],  [energy_pert_res_on_real]])\n",
    "# values = 1- values\n",
    "cols = [\"Energy TSTR MAE\"]\n",
    "index = [\"Style Time (In Samples)\", \"Style Time (Perturbed)\"]\n",
    "\n",
    "df_energy_results = pd.DataFrame(index=index, columns=cols, data=values)\n",
    "\n",
    "df_energy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_stocks_results.merge(df_energy_results, left_index=True, right_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
